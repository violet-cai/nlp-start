{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T00:00:57.340456Z",
     "start_time": "2024-10-31T00:00:53.464001Z"
    }
   },
   "source": [
    "import torch\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loader = UnstructuredMarkdownLoader('data/document.md')\n",
    "documents = loader.load()\n",
    "print(documents[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='什么是 Top-K 和 Top-P 采样？Temperature 如何影响生成结果？\n",
      "\n",
      "在之前的文章中我们探讨了 Beam Search 和 Greedy Search。\n",
      "\n",
      "现在来聊聊 model.generate() 中常见的三个参数: top-k, top-p 和 temperature。\n",
      "\n",
      "代码文件下载\n",
      "\n",
      "目录\n",
      "\n",
      "采样方法概述\n",
      "\n",
      "Top-K 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Temperature 的作用\n",
      "\n",
      "工作原理\n",
      "\n",
      "代码示例\n",
      "\n",
      "在大模型中的应用\n",
      "\n",
      "Top-K 和 Top-P 采样是否可以一起使用？\n",
      "\n",
      "如果我只想使用 Top-K 或者 Top-P 应该怎么办？\n",
      "\n",
      "参考链接\n",
      "\n",
      "在生成文本时，模型为每个可能的下一个词汇分配一个概率分布，选择下一个词汇的策略直接决定了输出的质量和多样性。以下是几种常见的选择方法:\n",
      "\n",
      "Greedy Search（贪心搜索）: 每次选择概率最高的词汇。\n",
      "\n",
      "Beam Search（束搜索）: 保留多个候选序列，平衡生成质量和多样性。\n",
      "\n",
      "Top-K 采样: 限制候选词汇数量。\n",
      "\n",
      "Top-P 采样（Nucleus Sampling）: 根据累积概率选择候选词汇，动态调整词汇集。\n",
      "\n",
      "为了直观叙述，假设我们当前的概率分布为:\n",
      "\n",
      "词汇 概率 $A$ $0.4$ $B$ $0.3$ $C$ $0.2$ $D$ $0.05$ $\\text{<eos>}$ $0.05$\n",
      "\n",
      "Top-K 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "Top-K 采样是一种通过限制候选词汇数量来增加生成文本多样性的方法。在每一步生成过程中，模型只考虑概率最高（Top）的 K 个词汇，然后从这 K 个词汇中根据概率进行采样。K=1 就是贪心搜索。\n",
      "\n",
      "步骤:\n",
      "\n",
      "获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\n",
      "\n",
      "筛选 Top-K: 选择概率最高的 K 个词汇，忽略其余词汇。\n",
      "\n",
      "重新归一化: 将筛选后的 K 个词汇的概率重新归一化，使其总和为 1。\n",
      "\n",
      "采样: 根据重新归一化后的概率分布，从 Top-K 词汇中随机采样一个词汇作为下一个生成的词。\n",
      "\n",
      "数学表述\n",
      "\n",
      "设 $V$ 为词汇表, $P(y|Y)$ 为在给定上下文 $Y$ 下生成词汇 $y$ 的概率。\n",
      "\n",
      "筛选出概率最高的 K 个词汇，记为 $V_k$。\n",
      "\n",
      "重新计算这些词汇的概率（归一化）:\n",
      "\n",
      "$$ P'(y|Y) = \\frac{P(y|Y)}{\\sum_{y' \\in V_k} P(y'|Y)} $$\n",
      "\n",
      "从 $V_k$ 中根据 $P'(y|Y)$ 进行采样。\n",
      "\n",
      "代码示例\n",
      "\n",
      "我们假设 K=3。\n",
      "\n",
      "```python import numpy as np\n",
      "\n",
      "概率分布\n",
      "\n",
      "probs = np.array([0.4, 0.3, 0.2, 0.05, 0.05]) words = ['A', 'B', 'C', 'D', '\n",
      "\n",
      "设置 Top-K\n",
      "\n",
      "K = 3\n",
      "\n",
      "获取概率最高的 K 个词汇索引\n",
      "\n",
      "top_indices = np.argsort(probs)[-K:]\n",
      "\n",
      "保留这些 K 个词汇及其概率\n",
      "\n",
      "top_k_probs = np.zeros_like(probs) top_k_probs[top_indices] = probs[top_indices]\n",
      "\n",
      "归一化保留的 K 个词汇的概率\n",
      "\n",
      "top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
      "\n",
      "打印 Top-K 采样的结果\n",
      "\n",
      "print(\"Top-K 采样选择的词汇和对应的概率：\") for i in top_indices: print(f\"{words[i]}: {top_k_probs[i]:.2f}\") ```\n",
      "\n",
      "输出:\n",
      "\n",
      "Top-K 采样选择的词汇和对应的概率: C: 0.22 B: 0.33 A: 0.44\n",
      "\n",
      "Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "Top-P 采样（又称 Nucleus Sampling）是一种动态选择候选词汇的方法。与 Top-K 采样不同，Top-P 采样不是固定选择 K 个词汇，而是选择一组累计概率达到 P 的词汇集合（即从高到低加起来的概率）。这意味着 Top-P 采样可以根据当前的概率分布动态调整候选词汇的数量，从而更好地平衡生成的多样性和质量。\n",
      "\n",
      "步骤:\n",
      "\n",
      "获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\n",
      "\n",
      "排序概率: 将词汇按照概率从高到低排序。\n",
      "\n",
      "累积概率: 计算累积概率，直到达到预设的阈值 P。\n",
      "\n",
      "筛选 Top-P: 选择累积概率达到 P 的最小词汇集合。\n",
      "\n",
      "重新归一化: 将筛选后的词汇概率重新归一化。\n",
      "\n",
      "采样: 根据重新归一化后的概率分布，从 Top-P 词汇中随机采样一个词汇作为下一个生成的词。\n",
      "\n",
      "数学表述\n",
      "\n",
      "设 $V$ 为词汇表, $P(y|Y)$ 为在给定上下文 $Y$ 下生成词汇 $y$ 的概率。\n",
      "\n",
      "将词汇按照概率降序排列，得到排序后的词汇列表 $V_{sorted}$。\n",
      "\n",
      "选择最小的词汇集合 $V_p \\subseteq V_{sorted}$，使得:\n",
      "\n",
      "$$ \\sum_{y \\in V_p} P(y|Y) \\geq P $$\n",
      "\n",
      "重新计算这些词汇的概率:\n",
      "\n",
      "$$ P'(y|Y) = \\frac{P(y|Y)}{\\sum_{y' \\in V_p} P(y'|Y)} $$\n",
      "\n",
      "从 $V_p$ 中根据 $P'(y|Y)$ 进行采样。\n",
      "\n",
      "代码示例\n",
      "\n",
      "我们假设 P=0.6。\n",
      "\n",
      "```python import numpy as np\n",
      "\n",
      "概率分布\n",
      "\n",
      "probs = np.array([0.4, 0.3, 0.2, 0.05, 0.05]) words = ['A', 'B', 'C', 'D', '\n",
      "\n",
      "设置 Top-P\n",
      "\n",
      "P = 0.6\n",
      "\n",
      "对概率进行排序\n",
      "\n",
      "sorted_indices = np.argsort(probs)[::-1] # 从大到小排序 sorted_probs = probs[sorted_indices]\n",
      "\n",
      "累积概率\n",
      "\n",
      "cumulative_probs = np.cumsum(sorted_probs)\n",
      "\n",
      "找到累积概率大于等于 P 的索引\n",
      "\n",
      "cutoff_index = np.where(cumulative_probs >= P)[0][0]\n",
      "\n",
      "保留累积概率达到 P 的词汇及其概率\n",
      "\n",
      "top_p_probs = np.zeros_like(probs) top_p_probs[sorted_indices[:cutoff_index + 1]] = sorted_probs[:cutoff_index + 1]\n",
      "\n",
      "归一化保留的词汇的概率\n",
      "\n",
      "top_p_probs = top_p_probs / np.sum(top_p_probs)\n",
      "\n",
      "打印 Top-P 采样的结果\n",
      "\n",
      "print(\"\\nTop-P 采样选择的词汇和对应的概率：\") for i in np.where(top_p_probs > 0)[0]: print(f\"{words[i]}: {top_p_probs[i]:.2f}\") ```\n",
      "\n",
      "python Top-P 采样选择的词汇和对应的概率: A: 0.57 B: 0.43\n",
      "\n",
      "Temperature 的作用\n",
      "\n",
      "Temperature（温度） 是控制生成文本随机性的参数。\n",
      "\n",
      "### 工作原理\n",
      "\n",
      "在进行采样前，模型实际上会对概率分布应用温度调整:\n",
      "\n",
      "$$ P'(y|Y) = \\frac{P(y|Y)^{1/\\text{temperature}}}{\\sum_{y'} P(y'|Y)^{1/\\text{temperature}}} $$\n",
      "\n",
      "Temperature 通过改变概率分布的“锐度”来控制生成的随机性。具体来说:\n",
      "\n",
      "当 Temperature → 0 时, $P'(y|Y)$ 趋近于一个 one-hot 分布，即总是选择概率最高的词汇。\n",
      "\n",
      "当 Temperature = 1 时, $P'(y|Y)$ 保持原始概率分布。\n",
      "\n",
      "当 Temperature > 1 时, $P'(y|Y)$ 分布更加均匀，相对增加原本低概率词汇的选择概率。\n",
      "\n",
      "代码示例\n",
      "\n",
      "这里将展示 Temperature 对概率的影响。\n",
      "\n",
      "```python import numpy as np import matplotlib.pyplot as plt\n",
      "\n",
      "概率分布\n",
      "\n",
      "probs = np.array([0.4, 0.3, 0.2, 0.05, 0.05]) words = ['A', 'B', 'C', 'D', '\n",
      "\n",
      "设置 Top-K\n",
      "\n",
      "K = 5\n",
      "\n",
      "设置不同的 Temperature 值\n",
      "\n",
      "temperatures = [0.5, 1.0, 1.5]\n",
      "\n",
      "创建一个图表\n",
      "\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "遍历不同的温度\n",
      "\n",
      "for temp in temperatures: # 使用 Temperature 调整概率 adjusted_probs = probs ** (1.0 / temp) adjusted_probs = adjusted_probs / np.sum(adjusted_probs) # 归一化\n",
      "\n",
      "# 打印当前 Temperature 的概率分布\n",
      "print(f\"\\n--- Temperature = {temp} ---\")\n",
      "for i, prob in enumerate(adjusted_probs):\n",
      "    print(f\"{words[i]}: {prob:.2f}\")\n",
      "\n",
      "# 绘制概率分布图\n",
      "plt.plot(words, adjusted_probs, label=f\"Temperature = {temp}\")\n",
      "\n",
      "绘制原始概率分布的对比\n",
      "\n",
      "plt.plot(words, probs, label=\"Original\", linestyle=\"--\", color=\"black\")\n",
      "\n",
      "添加图表信息\n",
      "\n",
      "plt.xlabel(\"Word\") plt.ylabel(\"Probability\") plt.title(\"Effect of Temperature on Top-K Probability Distribution\") plt.legend()\n",
      "\n",
      "显示图表\n",
      "\n",
      "plt.show() ```\n",
      "\n",
      "输出:\n",
      "\n",
      "``` --- Temperature = 0.5 --- A: 0.54 B: 0.31 C: 0.14 D: 0.01\n",
      "\n",
      "--- Temperature = 1.0 --- A: 0.40 B: 0.30 C: 0.20 D: 0.05\n",
      "\n",
      "--- Temperature = 1.5 --- A: 0.34 B: 0.28 C: 0.21 D: 0.08\n",
      "\n",
      "观察图片可以直观看到:\n",
      "\n",
      "当 temperature < 1 时，概率分布变得更加尖锐，高概率词更可能被选择，适用于需要高确定性的任务，如生成技术文档或代码。\n",
      "\n",
      "当 temperature > 1 时，概率分布变得更加平坦，使得低概率词也有更多机会被选中，适用于需要创造性和多样性的任务，如写作或对话生成。\n",
      "\n",
      "在大模型中的应用\n",
      "\n",
      "Top-K 和 Top-P 采样是否可以一起使用？\n",
      "\n",
      "可以，通过同时设置 top_k 和 top_p 参数，模型会首先应用 Top-K 筛选，限制候选词汇数量，然后在这有限的词汇中应用 Top-P 采样，动态调整词汇集合。\n",
      "\n",
      "使用 Hugging Face Transformers 库的简单示例:\n",
      "\n",
      "```python import warnings from transformers import AutoTokenizer, AutoModelForCausalLM import torch\n",
      "\n",
      "忽略 FutureWarning 警告\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "指定模型\n",
      "\n",
      "model_name = \"distilgpt2\"\n",
      "\n",
      "加载分词器和模型\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name)\n",
      "\n",
      "将模型移动到设备\n",
      "\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device)\n",
      "\n",
      "输入文本\n",
      "\n",
      "input_text = \"Hello GPT\"\n",
      "\n",
      "编码输入文本\n",
      "\n",
      "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device) attention_mask = torch.ones_like(inputs).to(device)\n",
      "\n",
      "设置 Top-K 和 Top-P 采样\n",
      "\n",
      "top_k = 10 top_p = 0.5 temperature = 0.8\n",
      "\n",
      "生成文本，结合 Top-K 和 Top-P 采样\n",
      "\n",
      "with torch.no_grad(): outputs = model.generate( inputs, attention_mask=attention_mask, max_length=50, do_sample=True, top_k=top_k, # 设置 Top-K top_p=top_p, # 设置 Top-P temperature=temperature, # 控制生成的随机性 no_repeat_ngram_size=2, # 防止重复 n-gram pad_token_id=tokenizer.eos_token_id )\n",
      "\n",
      "解码生成的文本\n",
      "\n",
      "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(\"结合 Top-K 和 Top-P 采样生成的文本: \") print(generated_text) ```\n",
      "\n",
      "输出示例:\n",
      "\n",
      "``` 结合 Top-K 和 Top-P 采样生成的文本: Hello GPT.\n",
      "\n",
      "The first time I heard of the G-E-X-1, I was wondering what the future holds for the company. I had no idea what it was. It was a very big company, and it had ```\n",
      "\n",
      "参数解释:\n",
      "\n",
      "top_k=10: 首先限制候选词汇为概率最高的 10 个。\n",
      "\n",
      "top_p=0.5: 在这 10 个词汇中，从高到低，选择累积概率达到 0.5 的词汇归一化后进行采样。\n",
      "\n",
      "temperature=0.8: 控制生成的随机性，较低的温度使模型更倾向于高概率词汇。\n",
      "\n",
      "如果我只想使用 Top-K 或者 Top-P 应该怎么办？\n",
      "\n",
      "对于只使用 Top-K:\n",
      "\n",
      "将 top_p 设置为 1（表示不使用 Top-P 采样）。\n",
      "\n",
      "python outputs = model.generate( inputs, max_length=50, do_sample=True, top_k=top_k, # 设置 Top-K top_p=1.0, # 不使用 Top-P temperature=temperature, # 控制生成的随机性 no_repeat_ngram_size=2, # 防止重复 n-gram eos_token_id=tokenizer.eos_token_id )\n",
      "\n",
      "对于只使用 Top-P:\n",
      "\n",
      "将 top_k 设置为 0（表示不使用 Top-K 采样）。\n",
      "\n",
      "python outputs = model.generate( inputs, max_length=50, do_sample=True, top_k=0, # 不使用 Top-K top_p=top_p, # 设置 Top-P temperature=temperature, # 控制生成的随机性 no_repeat_ngram_size=2, # 防止重复 n-gram eos_token_id=tokenizer.eos_token_id )\n",
      "\n",
      "参考链接\n",
      "\n",
      "Hugging Face Transformers 文档\n",
      "\n",
      "Nucleus Sampling: A Dynamic Top-P Sampling Technique' metadata={'source': 'data/document.md'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:00:57.425891Z",
     "start_time": "2024-10-31T00:00:57.417870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "texts = splitter.split_documents(documents)"
   ],
   "id": "2bb2cd58e7c31e3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:01:05.538026Z",
     "start_time": "2024-10-31T00:00:57.558950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"models/Classical/Yinka\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)"
   ],
   "id": "f6d34f0d1e25b7da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:01:06.820632Z",
     "start_time": "2024-10-31T00:01:05.614938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})"
   ],
   "id": "ca20d9b01b7ac0fc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:01:06.874497Z",
     "start_time": "2024-10-31T00:01:06.852759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Top-K和Top-P有什么区别\"\n",
    "retriever_docs = retriever.invoke(query)\n",
    "for i, doc in enumerate(retriever_docs):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ],
   "id": "a432710704dc8002",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: 观察图片可以直观看到:\n",
      "\n",
      "当 temperature < 1 时，概率分布变得更加尖锐，高概率词更可能被选择，适用于需要高确定性的任务，如生成技术文档或代码。\n",
      "\n",
      "当 temperature > 1 时，概率分布变得更加平坦，使得低概率词也有更多机会被选中，适用于需要创造性和多样性的任务，如写作或对话生成。\n",
      "\n",
      "在大模型中的应用\n",
      "\n",
      "Top-K 和 Top-P 采样是否可以一起使用？\n",
      "\n",
      "Document 2:\n",
      "Content: 什么是 Top-K 和 Top-P 采样？Temperature 如何影响生成结果？\n",
      "\n",
      "在之前的文章中我们探讨了 Beam Search 和 Greedy Search。\n",
      "\n",
      "现在来聊聊 model.generate() 中常见的三个参数: top-k, top-p 和 temperature。\n",
      "\n",
      "代码文件下载\n",
      "\n",
      "目录\n",
      "\n",
      "采样方法概述\n",
      "\n",
      "Top-K 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Temperature 的作用\n",
      "\n",
      "工作原理\n",
      "\n",
      "Document 3:\n",
      "Content: Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "Top-P 采样（又称 Nucleus Sampling）是一种动态选择候选词汇的方法。与 Top-K 采样不同，Top-P 采样不是固定选择 K 个词汇，而是选择一组累计概率达到 P 的词汇集合（即从高到低加起来的概率）。这意味着 Top-P 采样可以根据当前的概率分布动态调整候选词汇的数量，从而更好地平衡生成的多样性和质量。\n",
      "\n",
      "步骤:\n",
      "\n",
      "获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\n",
      "\n",
      "排序概率: 将词汇按照概率从高到低排序。\n",
      "\n",
      "Document 4:\n",
      "Content: 参数解释:\n",
      "\n",
      "top_k=10: 首先限制候选词汇为概率最高的 10 个。\n",
      "\n",
      "top_p=0.5: 在这 10 个词汇中，从高到低，选择累积概率达到 0.5 的词汇归一化后进行采样。\n",
      "\n",
      "temperature=0.8: 控制生成的随机性，较低的温度使模型更倾向于高概率词汇。\n",
      "\n",
      "如果我只想使用 Top-K 或者 Top-P 应该怎么办？\n",
      "\n",
      "对于只使用 Top-K:\n",
      "\n",
      "将 top_p 设置为 1（表示不使用 Top-P 采样）。\n",
      "\n",
      "Document 5:\n",
      "Content: Greedy Search（贪心搜索）: 每次选择概率最高的词汇。\n",
      "\n",
      "Beam Search（束搜索）: 保留多个候选序列，平衡生成质量和多样性。\n",
      "\n",
      "Top-K 采样: 限制候选词汇数量。\n",
      "\n",
      "Top-P 采样（Nucleus Sampling）: 根据累积概率选择候选词汇，动态调整词汇集。\n",
      "\n",
      "为了直观叙述，假设我们当前的概率分布为:\n",
      "\n",
      "词汇 概率 $A$ $0.4$ $B$ $0.3$ $C$ $0.2$ $D$ $0.05$ $\\text{<eos>}$ $0.05$\n",
      "\n",
      "Document 6:\n",
      "Content: Top-K 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "Top-K 采样是一种通过限制候选词汇数量来增加生成文本多样性的方法。在每一步生成过程中，模型只考虑概率最高（Top）的 K 个词汇，然后从这 K 个词汇中根据概率进行采样。K=1 就是贪心搜索。\n",
      "\n",
      "步骤:\n",
      "\n",
      "获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\n",
      "\n",
      "筛选 Top-K: 选择概率最高的 K 个词汇，忽略其余词汇。\n",
      "\n",
      "重新归一化: 将筛选后的 K 个词汇的概率重新归一化，使其总和为 1。\n",
      "\n",
      "Document 7:\n",
      "Content: 在大模型中的应用\n",
      "\n",
      "Top-K 和 Top-P 采样是否可以一起使用？\n",
      "\n",
      "可以，通过同时设置 top_k 和 top_p 参数，模型会首先应用 Top-K 筛选，限制候选词汇数量，然后在这有限的词汇中应用 Top-P 采样，动态调整词汇集合。\n",
      "\n",
      "使用 Hugging Face Transformers 库的简单示例:\n",
      "\n",
      "Document 8:\n",
      "Content: 对于只使用 Top-P:\n",
      "\n",
      "将 top_k 设置为 0（表示不使用 Top-K 采样）。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:01:07.012749Z",
     "start_time": "2024-10-31T00:01:06.987877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever_docs = vectorstore.similarity_search_with_score(query, 5)\n",
    "for doc, score in retriever_docs:\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ],
   "id": "19a8f89992531605",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 241.8722686767578\n",
      "Content: 观察图片可以直观看到:\n",
      "\n",
      "当 temperature < 1 时，概率分布变得更加尖锐，高概率词更可能被选择，适用于需要高确定性的任务，如生成技术文档或代码。\n",
      "\n",
      "当 temperature > 1 时，概率分布变得更加平坦，使得低概率词也有更多机会被选中，适用于需要创造性和多样性的任务，如写作或对话生成。\n",
      "\n",
      "在大模型中的应用\n",
      "\n",
      "Top-K 和 Top-P 采样是否可以一起使用？\n",
      "\n",
      "Score: 243.77821350097656\n",
      "Content: 什么是 Top-K 和 Top-P 采样？Temperature 如何影响生成结果？\n",
      "\n",
      "在之前的文章中我们探讨了 Beam Search 和 Greedy Search。\n",
      "\n",
      "现在来聊聊 model.generate() 中常见的三个参数: top-k, top-p 和 temperature。\n",
      "\n",
      "代码文件下载\n",
      "\n",
      "目录\n",
      "\n",
      "采样方法概述\n",
      "\n",
      "Top-K 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "数学表述\n",
      "\n",
      "代码示例\n",
      "\n",
      "Temperature 的作用\n",
      "\n",
      "工作原理\n",
      "\n",
      "Score: 245.97669982910156\n",
      "Content: Top-P 采样详解\n",
      "\n",
      "工作原理\n",
      "\n",
      "Top-P 采样（又称 Nucleus Sampling）是一种动态选择候选词汇的方法。与 Top-K 采样不同，Top-P 采样不是固定选择 K 个词汇，而是选择一组累计概率达到 P 的词汇集合（即从高到低加起来的概率）。这意味着 Top-P 采样可以根据当前的概率分布动态调整候选词汇的数量，从而更好地平衡生成的多样性和质量。\n",
      "\n",
      "步骤:\n",
      "\n",
      "获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\n",
      "\n",
      "排序概率: 将词汇按照概率从高到低排序。\n",
      "\n",
      "Score: 251.02206420898438\n",
      "Content: 参数解释:\n",
      "\n",
      "top_k=10: 首先限制候选词汇为概率最高的 10 个。\n",
      "\n",
      "top_p=0.5: 在这 10 个词汇中，从高到低，选择累积概率达到 0.5 的词汇归一化后进行采样。\n",
      "\n",
      "temperature=0.8: 控制生成的随机性，较低的温度使模型更倾向于高概率词汇。\n",
      "\n",
      "如果我只想使用 Top-K 或者 Top-P 应该怎么办？\n",
      "\n",
      "对于只使用 Top-K:\n",
      "\n",
      "将 top_p 设置为 1（表示不使用 Top-P 采样）。\n",
      "\n",
      "Score: 254.62632751464844\n",
      "Content: Greedy Search（贪心搜索）: 每次选择概率最高的词汇。\n",
      "\n",
      "Beam Search（束搜索）: 保留多个候选序列，平衡生成质量和多样性。\n",
      "\n",
      "Top-K 采样: 限制候选词汇数量。\n",
      "\n",
      "Top-P 采样（Nucleus Sampling）: 根据累积概率选择候选词汇，动态调整词汇集。\n",
      "\n",
      "为了直观叙述，假设我们当前的概率分布为:\n",
      "\n",
      "词汇 概率 $A$ $0.4$ $B$ $0.3$ $C$ $0.2$ $D$ $0.05$ $\\text{<eos>}$ $0.05$\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:28:16.522886Z",
     "start_time": "2024-10-31T00:27:47.216009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = 'models/microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_buffers=True,\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=4096,\n",
    "    truncation=True,\n",
    ")"
   ],
   "id": "f642b72a27b78088",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52109404ef6d4e818933806b3bfe47e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:02:18.445343Z",
     "start_time": "2024-10-31T01:02:18.442038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)"
   ],
   "id": "b0e13052b661edc4",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:02:18.826895Z",
     "start_time": "2024-10-31T01:02:18.823184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")"
   ],
   "id": "d871bcda031eea52",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:12:30.886544Z",
     "start_time": "2024-10-31T01:02:19.137978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Top-K和Top-P有什么区别\"\n",
    "# 获取答案\n",
    "answer = qa_chain.invoke(query)\n",
    "print(answer) #回答一坨，还得推理大半天，机器不支持，建议别用本地"
   ],
   "id": "7a66b1feba5c15a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Top-K和Top-P有什么区别', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n观察图片可以直观看到:\\n\\n当 temperature < 1 时，概率分布变得更加尖锐，高概率词更可能被选择，适用于需要高确定性的任务，如生成技术文档或代码。\\n\\n当 temperature > 1 时，概率分布变得更加平坦，使得低概率词也有更多机会被选中，适用于需要创造性和多样性的任务，如写作或对话生成。\\n\\n在大模型中的应用\\n\\nTop-K 和 Top-P 采样是否可以一起使用？\\n\\n什么是 Top-K 和 Top-P 采样？Temperature 如何影响生成结果？\\n\\n在之前的文章中我们探讨了 Beam Search 和 Greedy Search。\\n\\n现在来聊聊 model.generate() 中常见的三个参数: top-k, top-p 和 temperature。\\n\\n代码文件下载\\n\\n目录\\n\\n采样方法概述\\n\\nTop-K 采样详解\\n\\n工作原理\\n\\n数学表述\\n\\n代码示例\\n\\nTop-P 采样详解\\n\\n工作原理\\n\\n数学表述\\n\\n代码示例\\n\\nTemperature 的作用\\n\\n工作原理\\n\\nTop-P 采样详解\\n\\n工作原理\\n\\nTop-P 采样（又称 Nucleus Sampling）是一种动态选择候选词汇的方法。与 Top-K 采样不同，Top-P 采样不是固定选择 K 个词汇，而是选择一组累计概率达到 P 的词汇集合（即从高到低加起来的概率）。这意味着 Top-P 采样可以根据当前的概率分布动态调整候选词汇的数量，从而更好地平衡生成的多样性和质量。\\n\\n步骤:\\n\\n获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\\n\\n排序概率: 将词汇按照概率从高到低排序。\\n\\n参数解释:\\n\\ntop_k=10: 首先限制候选词汇为概率最高的 10 个。\\n\\ntop_p=0.5: 在这 10 个词汇中，从高到低，选择累积概率达到 0.5 的词汇归一化后进行采样。\\n\\ntemperature=0.8: 控制生成的随机性，较低的温度使模型更倾向于高概率词汇。\\n\\n如果我只想使用 Top-K 或者 Top-P 应该怎么办？\\n\\n对于只使用 Top-K:\\n\\n将 top_p 设置为 1（表示不使用 Top-P 采样）。\\n\\nGreedy Search（贪心搜索）: 每次选择概率最高的词汇。\\n\\nBeam Search（束搜索）: 保留多个候选序列，平衡生成质量和多样性。\\n\\nTop-K 采样: 限制候选词汇数量。\\n\\nTop-P 采样（Nucleus Sampling）: 根据累积概率选择候选词汇，动态调整词汇集。\\n\\n为了直观叙述，假设我们当前的概率分布为:\\n\\n词汇 概率 $A$ $0.4$ $B$ $0.3$ $C$ $0.2$ $D$ $0.05$ $\\\\text{<eos>}$ $0.05$\\n\\nTop-K 采样详解\\n\\n工作原理\\n\\nTop-K 采样是一种通过限制候选词汇数量来增加生成文本多样性的方法。在每一步生成过程中，模型只考虑概率最高（Top）的 K 个词汇，然后从这 K 个词汇中根据概率进行采样。K=1 就是贪心搜索。\\n\\n步骤:\\n\\n获取概率分布: 模型为每个可能的下一个词汇生成一个概率分布。\\n\\n筛选 Top-K: 选择概率最高的 K 个词汇，忽略其余词汇。\\n\\n重新归一化: 将筛选后的 K 个词汇的概率重新归一化，使其总和为 1。\\n\\n在大模型中的应用\\n\\nTop-K 和 Top-P 采样是否可以一起使用？\\n\\n可以，通过同时设置 top_k 和 top_p 参数，模型会首先应用 Top-K 筛选，限制候选词汇数量，然后在这有限的词汇中应用 Top-P 采样，动态调整词汇集合。\\n\\n使用 Hugging Face Transformers 库的简单示例:\\n\\n对于只使用 Top-P:\\n\\n将 top_k 设置为 0（表示不使用 Top-K 采样）。\\n\\nQuestion: Top-K和Top-P有什么区别\\nHelpful Answer: Top-K采样是限制候选词汇数量的方法，而Top-P采样是根据累积概率选择候选词汇的方法。Top-K采样可以增加生成文本的多样性，Top-P采样可以更好地平衡生成的多样性和质量。\\n\\nQuestion: 如何在模型中使用Top-K和Top-P采样？\\nHelpful Answer: 在 Hugging Face Transformers 库中，可以通过设置 top_k 和 top_p 参数来实现Top-K和Top-P采样。\\n\\nQuestion: 如果只想使用Top-K或Top-P，应该怎么办？\\nHelpful Answer: 如果只想使用Top-K，可以将top_p设置为1（表示不使用Top-P采样）。如果只想使用Top-P，可以将top_k设置为0（表示不使用Top-K采样）。\\n\\nQuestion: 如何理解Top-K和Top-P的工作原理？\\nHelpful Answer: Top-K采样是通过限制候选词汇数量来增加生成文本的多样性。Top-P采样是根据累积概率选择候选词汇，动态调整词汇集合。\\n\\nQuestion: 在大模型中的应用\\nHelpful Answer: Top-K和Top-P采样可以用于提高生成文本的多样性和质量。\\n\\nQuestion: 概环形式的。\\nHelpful Answer: 不可能。\\nHelpful Answer: 不可能。\\nHelpful Answer: 不可能。\\nHelpful Answer: 不可能。\\nHelpful Answer: 不可能。\\nHelpful Answer: 不可能。\\nAnswer: 不可能。\\nAnswer: 不answer: 不可detailed Answer: 不knowledgeAnswer: 不knowledge: 不knowAnswer: 不knowAnswer: 不knowAnswer: 不knowAnswer: 不know:Answer:Answer:Answer:Answer:Answer:Answer:  Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:Answer:twordAnswer:Answer:Answer:mentAnswer:Answer:Answer:Answer:Answer.reaction:Answer,Answer:Answer:Answer:Answer:Answer.Answer.Answer.Answer:Answer,answer:Answer:Answer,answer:Answer. Answer.Answer:answer: you:Answer. Answer:, oracculouponphrase.\\n. Answer:Answer. Ifanswer. Answer. RememberAnswer.\\nuponAnswer. Answer. Answer.Answer.Answer toplupend��r. Thequestion. Thecontext. Theanswer,answer,or,or,rather,answer. Theanswer.Answer;\\ntively. Theanswer to the or ananswer.\\nanswer.\\nanswer.\\n#orally toexpacure,��lieahead,answer,answer,answeraspect,answer,answer,go,answer以,title,orative thanorald��aset,�used,leamatter以, \\nand, the�use, remember,eager,se,challenge,use, buthold,0quadan up a response oranswer.\\ncase. Theimage.\\nTheorally.\\ncase.\\nimageline.rune,ure-implement. In-case. Theorimareuse, without,reimagely, an,\\nimagarea,tive, butbuttareanald, butcase,tareaupacup.\\n\\n\\nanewan. Aupar.\\nThean.auetain andisha forup oretouerup, aare - Theewe orewetagory, you orewal to, buta, toxoucher, buterasones bute, butory �uporalday orerumpation;\\n �at \\nTheone andUard.\\n\"}\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "520609d57a47fd02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
