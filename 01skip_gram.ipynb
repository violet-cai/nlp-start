{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.206959Z",
     "start_time": "2024-10-24T03:14:56.202618Z"
    }
   },
   "source": [
    "from sympy.printing.pretty.pretty_symbology import center\n",
    "\n",
    "sentence1 = 'Jane wants to go to Shenzhen .'\n",
    "sentence2 = 'Bob wants to go to Shanghai , me too .'\n",
    "token1 = sentence1.split(' ')\n",
    "token2 = sentence2.split(' ')\n",
    "print(\"token1 is {}\".format(token1))\n",
    "print(\"token2 is {}\".format(token2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token1 is ['Jane', 'wants', 'to', 'go', 'to', 'Shenzhen', '.']\n",
      "token2 is ['Bob', 'wants', 'to', 'go', 'to', 'Shanghai', ',', 'me', 'too', '.']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.274880Z",
     "start_time": "2024-10-24T03:14:56.270909Z"
    }
   },
   "source": [
    "# 向量化\n",
    "def vectorize_sentence(tokens, filtered_vocab):\n",
    "    vector = []\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "\n",
    "# 去重\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "\n",
    "# 停用词\n",
    "stopwords = [\"to\", \"is\", \"a\"]\n",
    "# 标点符号\n",
    "special_chars = [\",\", \":\", \";\", \".\", \"?\"]"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.333675Z",
     "start_time": "2024-10-24T03:14:56.328786Z"
    }
   },
   "source": [
    "filtered_tokens = []\n",
    "for w in unique(token1 + token2):\n",
    "    if w not in stopwords and w not in special_chars:\n",
    "        filtered_tokens.append(w)\n",
    "filtered_tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane', 'wants', 'go', 'Shenzhen', 'Bob', 'Shanghai', 'me', 'too']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.427108Z",
     "start_time": "2024-10-24T03:14:56.423854Z"
    }
   },
   "source": [
    "vector1 = vectorize_sentence(token1, filtered_tokens)\n",
    "vector2 = vectorize_sentence(token2, filtered_tokens)\n",
    "print(vector1)\n",
    "print(vector2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模式 api"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.507319Z",
     "start_time": "2024-10-24T03:14:56.500480Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sentence1 = 'Jane wants to go to Shenzhen .'\n",
    "sentence2 = 'Bob wants to go to Shanghai , me too .'\n",
    "count_vec = CountVectorizer(ngram_range=(1, 1))\n",
    "#transform\n",
    "feature = count_vec.fit_transform([sentence1, sentence2])\n",
    "\n",
    "#create dataframe\n",
    "df = pd.DataFrame(feature.toarray(), columns=count_vec.get_feature_names_out())\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   bob  go  jane  me  shanghai  shenzhen  to  too  wants\n",
       "0    0   1     1   0         0         1   2    0      1\n",
       "1    1   1     0   1         1         0   2    1      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bob</th>\n",
       "      <th>go</th>\n",
       "      <th>jane</th>\n",
       "      <th>me</th>\n",
       "      <th>shanghai</th>\n",
       "      <th>shenzhen</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.587180Z",
     "start_time": "2024-10-24T03:14:56.575179Z"
    }
   },
   "source": [
    "import jieba\n",
    "\n",
    "sentence1 = '李四爱去深圳'\n",
    "sentence2 = '张三很爱去上海，我也是'\n",
    "contents = [sentence1, sentence2]\n",
    "vec = TfidfVectorizer(tokenizer=jieba.lcut,\n",
    "                      stop_words=stopwords,\n",
    "                      norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "feature = vec.fit_transform(contents)  #直接对文档进行转换提取tfidf特征\n",
    "#一步就得到了tfidf向量\n",
    "print(feature.toarray())\n",
    "#create dataframe\n",
    "df = pd.DataFrame(feature.toarray(), columns=vec.get_feature_names_out())\n",
    "df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.57735027 0.57735027 0.         0.57735027 0.        ]\n",
      " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "  0.33333333 0.         0.         0.33333333 0.         0.33333333]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         上海         也         去        张三         很         我         是  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.333333  0.333333  0.333333  0.333333  0.333333  0.333333  0.333333   \n",
       "\n",
       "        李四       深圳         爱       爱去         ，  \n",
       "0  0.57735  0.57735  0.000000  0.57735  0.000000  \n",
       "1  0.00000  0.00000  0.333333  0.00000  0.333333  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>也</th>\n",
       "      <th>去</th>\n",
       "      <th>张三</th>\n",
       "      <th>很</th>\n",
       "      <th>我</th>\n",
       "      <th>是</th>\n",
       "      <th>李四</th>\n",
       "      <th>深圳</th>\n",
       "      <th>爱</th>\n",
       "      <th>爱去</th>\n",
       "      <th>，</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:56.665118Z",
     "start_time": "2024-10-24T03:14:56.662132Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_open(data_file):\n",
    "    f = open(data_file, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        line = json.loads(line.strip())\n",
    "        sentence.append(line['sentence'])\n",
    "    return sentence"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:14:57.024212Z",
     "start_time": "2024-10-24T03:14:56.715654Z"
    }
   },
   "source": [
    "train_data = get_open('./data/tnews/train.json')\n",
    "test_data = get_open('./data/tnews/test.json')\n",
    "dev_data = get_open('./data/tnews/dev.json')"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:15:01.014732Z",
     "start_time": "2024-10-24T03:14:57.049390Z"
    }
   },
   "source": [
    "sentences = train_data + test_data + dev_data\n",
    "words = [list(jieba.cut(sentence)) for sentence in sentences]"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:15:01.023056Z",
     "start_time": "2024-10-24T03:15:01.020247Z"
    }
   },
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:15:38.095552Z",
     "start_time": "2024-10-24T03:15:01.086619Z"
    }
   },
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(words, vector_size=4, window=3, min_count=1, workers=4, epochs=10)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 11:15:01,088 : INFO : collecting all words and their counts\n",
      "2024-10-24 11:15:01,089 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-10-24 11:15:01,104 : INFO : PROGRESS: at sentence #10000, processed 129141 words, keeping 25698 word types\n",
      "2024-10-24 11:15:01,119 : INFO : PROGRESS: at sentence #20000, processed 258779 words, keeping 39123 word types\n",
      "2024-10-24 11:15:01,136 : INFO : PROGRESS: at sentence #30000, processed 387255 words, keeping 49324 word types\n",
      "2024-10-24 11:15:01,157 : INFO : PROGRESS: at sentence #40000, processed 515598 words, keeping 57864 word types\n",
      "2024-10-24 11:15:01,178 : INFO : PROGRESS: at sentence #50000, processed 644895 words, keeping 65382 word types\n",
      "2024-10-24 11:15:01,201 : INFO : PROGRESS: at sentence #60000, processed 775338 words, keeping 74072 word types\n",
      "2024-10-24 11:15:01,225 : INFO : PROGRESS: at sentence #70000, processed 906119 words, keeping 80406 word types\n",
      "2024-10-24 11:15:01,235 : INFO : collected 82315 word types from a corpus of 949783 raw words and 73360 sentences\n",
      "2024-10-24 11:15:01,236 : INFO : Creating a fresh vocabulary\n",
      "2024-10-24 11:15:01,378 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 82315 unique words (100.00% of original 82315, drops 0)', 'datetime': '2024-10-24T11:15:01.378419', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:01,379 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 949783 word corpus (100.00% of original 949783, drops 0)', 'datetime': '2024-10-24T11:15:01.379307', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:01,572 : INFO : deleting the raw counts dictionary of 82315 items\n",
      "2024-10-24 11:15:01,574 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2024-10-24 11:15:01,574 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 804142.7803455585 word corpus (84.7%% of prior 949783)', 'datetime': '2024-10-24T11:15:01.574847', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:01,957 : INFO : estimated required memory for 82315 words, 2000000 buckets and 4 dimensions: 85609280 bytes\n",
      "2024-10-24 11:15:01,958 : INFO : resetting layer weights\n",
      "2024-10-24 11:15:02,486 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-24T11:15:02.486778', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2024-10-24 11:15:02,487 : INFO : FastText lifecycle event {'msg': 'training model with 4 workers on 82315 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2024-10-24T11:15:02.487799', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2024-10-24 11:15:02,850 : INFO : EPOCH 0: training on 949783 raw words (804525 effective words) took 0.4s, 2245819 effective words/s\n",
      "2024-10-24 11:15:03,271 : INFO : EPOCH 1: training on 949783 raw words (803861 effective words) took 0.4s, 1931893 effective words/s\n",
      "2024-10-24 11:15:03,665 : INFO : EPOCH 2: training on 949783 raw words (804048 effective words) took 0.4s, 2064387 effective words/s\n",
      "2024-10-24 11:15:04,026 : INFO : EPOCH 3: training on 949783 raw words (804112 effective words) took 0.4s, 2254138 effective words/s\n",
      "2024-10-24 11:15:04,363 : INFO : EPOCH 4: training on 949783 raw words (804445 effective words) took 0.3s, 2415991 effective words/s\n",
      "2024-10-24 11:15:04,735 : INFO : EPOCH 5: training on 949783 raw words (804078 effective words) took 0.4s, 2182608 effective words/s\n",
      "2024-10-24 11:15:05,186 : INFO : EPOCH 6: training on 949783 raw words (804171 effective words) took 0.4s, 1800327 effective words/s\n",
      "2024-10-24 11:15:05,615 : INFO : EPOCH 7: training on 949783 raw words (803623 effective words) took 0.4s, 1893831 effective words/s\n",
      "2024-10-24 11:15:05,987 : INFO : EPOCH 8: training on 949783 raw words (804144 effective words) took 0.4s, 2186643 effective words/s\n",
      "2024-10-24 11:15:06,307 : INFO : EPOCH 9: training on 949783 raw words (804087 effective words) took 0.3s, 2547456 effective words/s\n",
      "2024-10-24 11:15:06,648 : INFO : EPOCH 10: training on 949783 raw words (803956 effective words) took 0.3s, 2384788 effective words/s\n",
      "2024-10-24 11:15:07,016 : INFO : EPOCH 11: training on 949783 raw words (804289 effective words) took 0.4s, 2213154 effective words/s\n",
      "2024-10-24 11:15:07,375 : INFO : EPOCH 12: training on 949783 raw words (804202 effective words) took 0.4s, 2262396 effective words/s\n",
      "2024-10-24 11:15:07,761 : INFO : EPOCH 13: training on 949783 raw words (804011 effective words) took 0.4s, 2107444 effective words/s\n",
      "2024-10-24 11:15:08,119 : INFO : EPOCH 14: training on 949783 raw words (804247 effective words) took 0.4s, 2272280 effective words/s\n",
      "2024-10-24 11:15:08,493 : INFO : EPOCH 15: training on 949783 raw words (804262 effective words) took 0.4s, 2177760 effective words/s\n",
      "2024-10-24 11:15:08,884 : INFO : EPOCH 16: training on 949783 raw words (804231 effective words) took 0.4s, 2086413 effective words/s\n",
      "2024-10-24 11:15:09,257 : INFO : EPOCH 17: training on 949783 raw words (804400 effective words) took 0.4s, 2179107 effective words/s\n",
      "2024-10-24 11:15:09,599 : INFO : EPOCH 18: training on 949783 raw words (803944 effective words) took 0.3s, 2381332 effective words/s\n",
      "2024-10-24 11:15:09,939 : INFO : EPOCH 19: training on 949783 raw words (804140 effective words) took 0.3s, 2397960 effective words/s\n",
      "2024-10-24 11:15:10,263 : INFO : EPOCH 20: training on 949783 raw words (804195 effective words) took 0.3s, 2517065 effective words/s\n",
      "2024-10-24 11:15:10,594 : INFO : EPOCH 21: training on 949783 raw words (804528 effective words) took 0.3s, 2458154 effective words/s\n",
      "2024-10-24 11:15:10,913 : INFO : EPOCH 22: training on 949783 raw words (804423 effective words) took 0.3s, 2557732 effective words/s\n",
      "2024-10-24 11:15:11,240 : INFO : EPOCH 23: training on 949783 raw words (803941 effective words) took 0.3s, 2487650 effective words/s\n",
      "2024-10-24 11:15:11,566 : INFO : EPOCH 24: training on 949783 raw words (804183 effective words) took 0.3s, 2499685 effective words/s\n",
      "2024-10-24 11:15:11,898 : INFO : EPOCH 25: training on 949783 raw words (804278 effective words) took 0.3s, 2454043 effective words/s\n",
      "2024-10-24 11:15:12,237 : INFO : EPOCH 26: training on 949783 raw words (803953 effective words) took 0.3s, 2407168 effective words/s\n",
      "2024-10-24 11:15:12,565 : INFO : EPOCH 27: training on 949783 raw words (803844 effective words) took 0.3s, 2480487 effective words/s\n",
      "2024-10-24 11:15:12,887 : INFO : EPOCH 28: training on 949783 raw words (804398 effective words) took 0.3s, 2532111 effective words/s\n",
      "2024-10-24 11:15:13,214 : INFO : EPOCH 29: training on 949783 raw words (804050 effective words) took 0.3s, 2491829 effective words/s\n",
      "2024-10-24 11:15:13,544 : INFO : EPOCH 30: training on 949783 raw words (804196 effective words) took 0.3s, 2461869 effective words/s\n",
      "2024-10-24 11:15:13,863 : INFO : EPOCH 31: training on 949783 raw words (804269 effective words) took 0.3s, 2557420 effective words/s\n",
      "2024-10-24 11:15:14,211 : INFO : EPOCH 32: training on 949783 raw words (804362 effective words) took 0.3s, 2339778 effective words/s\n",
      "2024-10-24 11:15:14,535 : INFO : EPOCH 33: training on 949783 raw words (804279 effective words) took 0.3s, 2510846 effective words/s\n",
      "2024-10-24 11:15:14,863 : INFO : EPOCH 34: training on 949783 raw words (803969 effective words) took 0.3s, 2484039 effective words/s\n",
      "2024-10-24 11:15:15,207 : INFO : EPOCH 35: training on 949783 raw words (804405 effective words) took 0.3s, 2364960 effective words/s\n",
      "2024-10-24 11:15:15,555 : INFO : EPOCH 36: training on 949783 raw words (804374 effective words) took 0.3s, 2338979 effective words/s\n",
      "2024-10-24 11:15:15,910 : INFO : EPOCH 37: training on 949783 raw words (804314 effective words) took 0.4s, 2295624 effective words/s\n",
      "2024-10-24 11:15:16,257 : INFO : EPOCH 38: training on 949783 raw words (804285 effective words) took 0.3s, 2346347 effective words/s\n",
      "2024-10-24 11:15:16,602 : INFO : EPOCH 39: training on 949783 raw words (804218 effective words) took 0.3s, 2366761 effective words/s\n",
      "2024-10-24 11:15:16,923 : INFO : EPOCH 40: training on 949783 raw words (804085 effective words) took 0.3s, 2534816 effective words/s\n",
      "2024-10-24 11:15:17,253 : INFO : EPOCH 41: training on 949783 raw words (804153 effective words) took 0.3s, 2468749 effective words/s\n",
      "2024-10-24 11:15:17,577 : INFO : EPOCH 42: training on 949783 raw words (804276 effective words) took 0.3s, 2513402 effective words/s\n",
      "2024-10-24 11:15:17,901 : INFO : EPOCH 43: training on 949783 raw words (804300 effective words) took 0.3s, 2514822 effective words/s\n",
      "2024-10-24 11:15:18,229 : INFO : EPOCH 44: training on 949783 raw words (803932 effective words) took 0.3s, 2479277 effective words/s\n",
      "2024-10-24 11:15:18,569 : INFO : EPOCH 45: training on 949783 raw words (804466 effective words) took 0.3s, 2401422 effective words/s\n",
      "2024-10-24 11:15:18,899 : INFO : EPOCH 46: training on 949783 raw words (803580 effective words) took 0.3s, 2462109 effective words/s\n",
      "2024-10-24 11:15:19,236 : INFO : EPOCH 47: training on 949783 raw words (804188 effective words) took 0.3s, 2412572 effective words/s\n",
      "2024-10-24 11:15:19,601 : INFO : EPOCH 48: training on 949783 raw words (803952 effective words) took 0.4s, 2231966 effective words/s\n",
      "2024-10-24 11:15:19,931 : INFO : EPOCH 49: training on 949783 raw words (804344 effective words) took 0.3s, 2471361 effective words/s\n",
      "2024-10-24 11:15:20,257 : INFO : EPOCH 50: training on 949783 raw words (804083 effective words) took 0.3s, 2495428 effective words/s\n",
      "2024-10-24 11:15:20,597 : INFO : EPOCH 51: training on 949783 raw words (803891 effective words) took 0.3s, 2393738 effective words/s\n",
      "2024-10-24 11:15:20,948 : INFO : EPOCH 52: training on 949783 raw words (804016 effective words) took 0.3s, 2323917 effective words/s\n",
      "2024-10-24 11:15:21,274 : INFO : EPOCH 53: training on 949783 raw words (803354 effective words) took 0.3s, 2491381 effective words/s\n",
      "2024-10-24 11:15:21,663 : INFO : EPOCH 54: training on 949783 raw words (804384 effective words) took 0.4s, 2095117 effective words/s\n",
      "2024-10-24 11:15:22,089 : INFO : EPOCH 55: training on 949783 raw words (804124 effective words) took 0.4s, 1904228 effective words/s\n",
      "2024-10-24 11:15:22,524 : INFO : EPOCH 56: training on 949783 raw words (803987 effective words) took 0.4s, 1866884 effective words/s\n",
      "2024-10-24 11:15:23,025 : INFO : EPOCH 57: training on 949783 raw words (804466 effective words) took 0.5s, 1620479 effective words/s\n",
      "2024-10-24 11:15:23,438 : INFO : EPOCH 58: training on 949783 raw words (804092 effective words) took 0.4s, 1967871 effective words/s\n",
      "2024-10-24 11:15:23,758 : INFO : EPOCH 59: training on 949783 raw words (804152 effective words) took 0.3s, 2552263 effective words/s\n",
      "2024-10-24 11:15:24,165 : INFO : EPOCH 60: training on 949783 raw words (804020 effective words) took 0.4s, 1999069 effective words/s\n",
      "2024-10-24 11:15:24,524 : INFO : EPOCH 61: training on 949783 raw words (804013 effective words) took 0.4s, 2263404 effective words/s\n",
      "2024-10-24 11:15:24,874 : INFO : EPOCH 62: training on 949783 raw words (804448 effective words) took 0.3s, 2328803 effective words/s\n",
      "2024-10-24 11:15:25,302 : INFO : EPOCH 63: training on 949783 raw words (804240 effective words) took 0.4s, 1900804 effective words/s\n",
      "2024-10-24 11:15:25,640 : INFO : EPOCH 64: training on 949783 raw words (804267 effective words) took 0.3s, 2414079 effective words/s\n",
      "2024-10-24 11:15:25,979 : INFO : EPOCH 65: training on 949783 raw words (804213 effective words) took 0.3s, 2403400 effective words/s\n",
      "2024-10-24 11:15:26,314 : INFO : EPOCH 66: training on 949783 raw words (804330 effective words) took 0.3s, 2424946 effective words/s\n",
      "2024-10-24 11:15:26,653 : INFO : EPOCH 67: training on 949783 raw words (803967 effective words) took 0.3s, 2407140 effective words/s\n",
      "2024-10-24 11:15:26,998 : INFO : EPOCH 68: training on 949783 raw words (804098 effective words) took 0.3s, 2355900 effective words/s\n",
      "2024-10-24 11:15:27,333 : INFO : EPOCH 69: training on 949783 raw words (804174 effective words) took 0.3s, 2431685 effective words/s\n",
      "2024-10-24 11:15:27,685 : INFO : EPOCH 70: training on 949783 raw words (804532 effective words) took 0.3s, 2311973 effective words/s\n",
      "2024-10-24 11:15:28,025 : INFO : EPOCH 71: training on 949783 raw words (804359 effective words) took 0.3s, 2397043 effective words/s\n",
      "2024-10-24 11:15:28,363 : INFO : EPOCH 72: training on 949783 raw words (804175 effective words) took 0.3s, 2409553 effective words/s\n",
      "2024-10-24 11:15:28,706 : INFO : EPOCH 73: training on 949783 raw words (804061 effective words) took 0.3s, 2374429 effective words/s\n",
      "2024-10-24 11:15:29,048 : INFO : EPOCH 74: training on 949783 raw words (804122 effective words) took 0.3s, 2381900 effective words/s\n",
      "2024-10-24 11:15:29,379 : INFO : EPOCH 75: training on 949783 raw words (804151 effective words) took 0.3s, 2461794 effective words/s\n",
      "2024-10-24 11:15:29,745 : INFO : EPOCH 76: training on 949783 raw words (804221 effective words) took 0.4s, 2223021 effective words/s\n",
      "2024-10-24 11:15:30,115 : INFO : EPOCH 77: training on 949783 raw words (804484 effective words) took 0.4s, 2196580 effective words/s\n",
      "2024-10-24 11:15:30,459 : INFO : EPOCH 78: training on 949783 raw words (803724 effective words) took 0.3s, 2363647 effective words/s\n",
      "2024-10-24 11:15:30,791 : INFO : EPOCH 79: training on 949783 raw words (804383 effective words) took 0.3s, 2461134 effective words/s\n",
      "2024-10-24 11:15:31,126 : INFO : EPOCH 80: training on 949783 raw words (804086 effective words) took 0.3s, 2426771 effective words/s\n",
      "2024-10-24 11:15:31,480 : INFO : EPOCH 81: training on 949783 raw words (803849 effective words) took 0.3s, 2301692 effective words/s\n",
      "2024-10-24 11:15:31,843 : INFO : EPOCH 82: training on 949783 raw words (804263 effective words) took 0.4s, 2242143 effective words/s\n",
      "2024-10-24 11:15:32,203 : INFO : EPOCH 83: training on 949783 raw words (803884 effective words) took 0.4s, 2254671 effective words/s\n",
      "2024-10-24 11:15:32,573 : INFO : EPOCH 84: training on 949783 raw words (804262 effective words) took 0.4s, 2203682 effective words/s\n",
      "2024-10-24 11:15:32,925 : INFO : EPOCH 85: training on 949783 raw words (804411 effective words) took 0.3s, 2308255 effective words/s\n",
      "2024-10-24 11:15:33,259 : INFO : EPOCH 86: training on 949783 raw words (804542 effective words) took 0.3s, 2441617 effective words/s\n",
      "2024-10-24 11:15:33,599 : INFO : EPOCH 87: training on 949783 raw words (803874 effective words) took 0.3s, 2396347 effective words/s\n",
      "2024-10-24 11:15:33,931 : INFO : EPOCH 88: training on 949783 raw words (804007 effective words) took 0.3s, 2456716 effective words/s\n",
      "2024-10-24 11:15:34,306 : INFO : EPOCH 89: training on 949783 raw words (804427 effective words) took 0.4s, 2167531 effective words/s\n",
      "2024-10-24 11:15:34,643 : INFO : EPOCH 90: training on 949783 raw words (804119 effective words) took 0.3s, 2418863 effective words/s\n",
      "2024-10-24 11:15:34,969 : INFO : EPOCH 91: training on 949783 raw words (803971 effective words) took 0.3s, 2495226 effective words/s\n",
      "2024-10-24 11:15:35,324 : INFO : EPOCH 92: training on 949783 raw words (804282 effective words) took 0.4s, 2291108 effective words/s\n",
      "2024-10-24 11:15:35,658 : INFO : EPOCH 93: training on 949783 raw words (804038 effective words) took 0.3s, 2438815 effective words/s\n",
      "2024-10-24 11:15:35,984 : INFO : EPOCH 94: training on 949783 raw words (804107 effective words) took 0.3s, 2504843 effective words/s\n",
      "2024-10-24 11:15:36,336 : INFO : EPOCH 95: training on 949783 raw words (804495 effective words) took 0.3s, 2310442 effective words/s\n",
      "2024-10-24 11:15:36,711 : INFO : EPOCH 96: training on 949783 raw words (804142 effective words) took 0.4s, 2171319 effective words/s\n",
      "2024-10-24 11:15:37,041 : INFO : EPOCH 97: training on 949783 raw words (804463 effective words) took 0.3s, 2466289 effective words/s\n",
      "2024-10-24 11:15:37,377 : INFO : EPOCH 98: training on 949783 raw words (804218 effective words) took 0.3s, 2422736 effective words/s\n",
      "2024-10-24 11:15:37,713 : INFO : EPOCH 99: training on 949783 raw words (804164 effective words) took 0.3s, 2426766 effective words/s\n",
      "2024-10-24 11:15:37,714 : INFO : FastText lifecycle event {'msg': 'training on 94978300 raw words (80416651 effective words) took 35.2s, 2282908 effective words/s', 'datetime': '2024-10-24T11:15:37.714005', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2024-10-24 11:15:38,093 : INFO : FastText lifecycle event {'params': 'FastText<vocab=82315, vector_size=4, alpha=0.025>', 'datetime': '2024-10-24T11:15:38.093399', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:16:03.677560Z",
     "start_time": "2024-10-24T03:15:38.116685Z"
    }
   },
   "cell_type": "code",
   "source": "model1 = word2vec.Word2Vec(words, vector_size=4, window=3, min_count=1, workers=4, epochs=10)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 11:15:38,118 : INFO : collecting all words and their counts\n",
      "2024-10-24 11:15:38,119 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-10-24 11:15:38,134 : INFO : PROGRESS: at sentence #10000, processed 129141 words, keeping 25698 word types\n",
      "2024-10-24 11:15:38,151 : INFO : PROGRESS: at sentence #20000, processed 258779 words, keeping 39123 word types\n",
      "2024-10-24 11:15:38,167 : INFO : PROGRESS: at sentence #30000, processed 387255 words, keeping 49324 word types\n",
      "2024-10-24 11:15:38,183 : INFO : PROGRESS: at sentence #40000, processed 515598 words, keeping 57864 word types\n",
      "2024-10-24 11:15:38,198 : INFO : PROGRESS: at sentence #50000, processed 644895 words, keeping 65382 word types\n",
      "2024-10-24 11:15:38,214 : INFO : PROGRESS: at sentence #60000, processed 775338 words, keeping 74072 word types\n",
      "2024-10-24 11:15:38,231 : INFO : PROGRESS: at sentence #70000, processed 906119 words, keeping 80406 word types\n",
      "2024-10-24 11:15:38,240 : INFO : collected 82315 word types from a corpus of 949783 raw words and 73360 sentences\n",
      "2024-10-24 11:15:38,241 : INFO : Creating a fresh vocabulary\n",
      "2024-10-24 11:15:38,380 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 82315 unique words (100.00% of original 82315, drops 0)', 'datetime': '2024-10-24T11:15:38.380592', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:38,381 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 949783 word corpus (100.00% of original 949783, drops 0)', 'datetime': '2024-10-24T11:15:38.381379', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:38,567 : INFO : deleting the raw counts dictionary of 82315 items\n",
      "2024-10-24 11:15:38,569 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2024-10-24 11:15:38,569 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 804142.7803455585 word corpus (84.7%% of prior 949783)', 'datetime': '2024-10-24T11:15:38.569947', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2024-10-24 11:15:38,851 : INFO : estimated required memory for 82315 words and 4 dimensions: 43791580 bytes\n",
      "2024-10-24 11:15:38,851 : INFO : resetting layer weights\n",
      "2024-10-24 11:15:38,853 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-24T11:15:38.853248', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2024-10-24 11:15:38,853 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 82315 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2024-10-24T11:15:38.853605', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2024-10-24 11:15:39,097 : INFO : EPOCH 0: training on 949783 raw words (803974 effective words) took 0.2s, 3356656 effective words/s\n",
      "2024-10-24 11:15:39,366 : INFO : EPOCH 1: training on 949783 raw words (804335 effective words) took 0.3s, 3034066 effective words/s\n",
      "2024-10-24 11:15:39,597 : INFO : EPOCH 2: training on 949783 raw words (804251 effective words) took 0.2s, 3556814 effective words/s\n",
      "2024-10-24 11:15:39,828 : INFO : EPOCH 3: training on 949783 raw words (803888 effective words) took 0.2s, 3537668 effective words/s\n",
      "2024-10-24 11:15:40,058 : INFO : EPOCH 4: training on 949783 raw words (804197 effective words) took 0.2s, 3558363 effective words/s\n",
      "2024-10-24 11:15:40,376 : INFO : EPOCH 5: training on 949783 raw words (803976 effective words) took 0.3s, 2565161 effective words/s\n",
      "2024-10-24 11:15:40,654 : INFO : EPOCH 6: training on 949783 raw words (804288 effective words) took 0.3s, 2936761 effective words/s\n",
      "2024-10-24 11:15:40,907 : INFO : EPOCH 7: training on 949783 raw words (804434 effective words) took 0.2s, 3237954 effective words/s\n",
      "2024-10-24 11:15:41,139 : INFO : EPOCH 8: training on 949783 raw words (804016 effective words) took 0.2s, 3530273 effective words/s\n",
      "2024-10-24 11:15:41,382 : INFO : EPOCH 9: training on 949783 raw words (804195 effective words) took 0.2s, 3370494 effective words/s\n",
      "2024-10-24 11:15:41,611 : INFO : EPOCH 10: training on 949783 raw words (804309 effective words) took 0.2s, 3573260 effective words/s\n",
      "2024-10-24 11:15:41,846 : INFO : EPOCH 11: training on 949783 raw words (804448 effective words) took 0.2s, 3492427 effective words/s\n",
      "2024-10-24 11:15:42,076 : INFO : EPOCH 12: training on 949783 raw words (804429 effective words) took 0.2s, 3560033 effective words/s\n",
      "2024-10-24 11:15:42,321 : INFO : EPOCH 13: training on 949783 raw words (803948 effective words) took 0.2s, 3344435 effective words/s\n",
      "2024-10-24 11:15:42,560 : INFO : EPOCH 14: training on 949783 raw words (804291 effective words) took 0.2s, 3418042 effective words/s\n",
      "2024-10-24 11:15:42,866 : INFO : EPOCH 15: training on 949783 raw words (804118 effective words) took 0.3s, 2662863 effective words/s\n",
      "2024-10-24 11:15:43,141 : INFO : EPOCH 16: training on 949783 raw words (804305 effective words) took 0.3s, 2974493 effective words/s\n",
      "2024-10-24 11:15:43,382 : INFO : EPOCH 17: training on 949783 raw words (803961 effective words) took 0.2s, 3392503 effective words/s\n",
      "2024-10-24 11:15:43,611 : INFO : EPOCH 18: training on 949783 raw words (803952 effective words) took 0.2s, 3587018 effective words/s\n",
      "2024-10-24 11:15:43,849 : INFO : EPOCH 19: training on 949783 raw words (803930 effective words) took 0.2s, 3428399 effective words/s\n",
      "2024-10-24 11:15:44,079 : INFO : EPOCH 20: training on 949783 raw words (804575 effective words) took 0.2s, 3562929 effective words/s\n",
      "2024-10-24 11:15:44,319 : INFO : EPOCH 21: training on 949783 raw words (804416 effective words) took 0.2s, 3406105 effective words/s\n",
      "2024-10-24 11:15:44,551 : INFO : EPOCH 22: training on 949783 raw words (804106 effective words) took 0.2s, 3537732 effective words/s\n",
      "2024-10-24 11:15:44,792 : INFO : EPOCH 23: training on 949783 raw words (804052 effective words) took 0.2s, 3385403 effective words/s\n",
      "2024-10-24 11:15:45,043 : INFO : EPOCH 24: training on 949783 raw words (804085 effective words) took 0.2s, 3255578 effective words/s\n",
      "2024-10-24 11:15:45,286 : INFO : EPOCH 25: training on 949783 raw words (804328 effective words) took 0.2s, 3368373 effective words/s\n",
      "2024-10-24 11:15:45,519 : INFO : EPOCH 26: training on 949783 raw words (803821 effective words) took 0.2s, 3506633 effective words/s\n",
      "2024-10-24 11:15:45,752 : INFO : EPOCH 27: training on 949783 raw words (804250 effective words) took 0.2s, 3515654 effective words/s\n",
      "2024-10-24 11:15:45,994 : INFO : EPOCH 28: training on 949783 raw words (804513 effective words) took 0.2s, 3383663 effective words/s\n",
      "2024-10-24 11:15:46,227 : INFO : EPOCH 29: training on 949783 raw words (804259 effective words) took 0.2s, 3508197 effective words/s\n",
      "2024-10-24 11:15:46,460 : INFO : EPOCH 30: training on 949783 raw words (804609 effective words) took 0.2s, 3511811 effective words/s\n",
      "2024-10-24 11:15:46,690 : INFO : EPOCH 31: training on 949783 raw words (804098 effective words) took 0.2s, 3550326 effective words/s\n",
      "2024-10-24 11:15:46,932 : INFO : EPOCH 32: training on 949783 raw words (803960 effective words) took 0.2s, 3385845 effective words/s\n",
      "2024-10-24 11:15:47,172 : INFO : EPOCH 33: training on 949783 raw words (803854 effective words) took 0.2s, 3420401 effective words/s\n",
      "2024-10-24 11:15:47,418 : INFO : EPOCH 34: training on 949783 raw words (804032 effective words) took 0.2s, 3320437 effective words/s\n",
      "2024-10-24 11:15:47,662 : INFO : EPOCH 35: training on 949783 raw words (804168 effective words) took 0.2s, 3346028 effective words/s\n",
      "2024-10-24 11:15:47,900 : INFO : EPOCH 36: training on 949783 raw words (804298 effective words) took 0.2s, 3446134 effective words/s\n",
      "2024-10-24 11:15:48,129 : INFO : EPOCH 37: training on 949783 raw words (804506 effective words) took 0.2s, 3573868 effective words/s\n",
      "2024-10-24 11:15:48,368 : INFO : EPOCH 38: training on 949783 raw words (804195 effective words) took 0.2s, 3425447 effective words/s\n",
      "2024-10-24 11:15:48,602 : INFO : EPOCH 39: training on 949783 raw words (804189 effective words) took 0.2s, 3490827 effective words/s\n",
      "2024-10-24 11:15:48,831 : INFO : EPOCH 40: training on 949783 raw words (804012 effective words) took 0.2s, 3573210 effective words/s\n",
      "2024-10-24 11:15:49,071 : INFO : EPOCH 41: training on 949783 raw words (804401 effective words) took 0.2s, 3414217 effective words/s\n",
      "2024-10-24 11:15:49,302 : INFO : EPOCH 42: training on 949783 raw words (804523 effective words) took 0.2s, 3542068 effective words/s\n",
      "2024-10-24 11:15:49,552 : INFO : EPOCH 43: training on 949783 raw words (804443 effective words) took 0.2s, 3280447 effective words/s\n",
      "2024-10-24 11:15:49,781 : INFO : EPOCH 44: training on 949783 raw words (804480 effective words) took 0.2s, 3575029 effective words/s\n",
      "2024-10-24 11:15:50,031 : INFO : EPOCH 45: training on 949783 raw words (804482 effective words) took 0.2s, 3269883 effective words/s\n",
      "2024-10-24 11:15:50,264 : INFO : EPOCH 46: training on 949783 raw words (804496 effective words) took 0.2s, 3524425 effective words/s\n",
      "2024-10-24 11:15:50,501 : INFO : EPOCH 47: training on 949783 raw words (803980 effective words) took 0.2s, 3456298 effective words/s\n",
      "2024-10-24 11:15:50,738 : INFO : EPOCH 48: training on 949783 raw words (804118 effective words) took 0.2s, 3445896 effective words/s\n",
      "2024-10-24 11:15:50,989 : INFO : EPOCH 49: training on 949783 raw words (804059 effective words) took 0.2s, 3265161 effective words/s\n",
      "2024-10-24 11:15:51,246 : INFO : EPOCH 50: training on 949783 raw words (804135 effective words) took 0.3s, 3169711 effective words/s\n",
      "2024-10-24 11:15:51,490 : INFO : EPOCH 51: training on 949783 raw words (804084 effective words) took 0.2s, 3348362 effective words/s\n",
      "2024-10-24 11:15:51,771 : INFO : EPOCH 52: training on 949783 raw words (803986 effective words) took 0.3s, 2912345 effective words/s\n",
      "2024-10-24 11:15:52,021 : INFO : EPOCH 53: training on 949783 raw words (803920 effective words) took 0.2s, 3267347 effective words/s\n",
      "2024-10-24 11:15:52,271 : INFO : EPOCH 54: training on 949783 raw words (804128 effective words) took 0.2s, 3255632 effective words/s\n",
      "2024-10-24 11:15:52,510 : INFO : EPOCH 55: training on 949783 raw words (803971 effective words) took 0.2s, 3430981 effective words/s\n",
      "2024-10-24 11:15:52,747 : INFO : EPOCH 56: training on 949783 raw words (804366 effective words) took 0.2s, 3455738 effective words/s\n",
      "2024-10-24 11:15:52,992 : INFO : EPOCH 57: training on 949783 raw words (804172 effective words) took 0.2s, 3322881 effective words/s\n",
      "2024-10-24 11:15:53,224 : INFO : EPOCH 58: training on 949783 raw words (804203 effective words) took 0.2s, 3537603 effective words/s\n",
      "2024-10-24 11:15:53,457 : INFO : EPOCH 59: training on 949783 raw words (804155 effective words) took 0.2s, 3512060 effective words/s\n",
      "2024-10-24 11:15:53,686 : INFO : EPOCH 60: training on 949783 raw words (804205 effective words) took 0.2s, 3586480 effective words/s\n",
      "2024-10-24 11:15:53,971 : INFO : EPOCH 61: training on 949783 raw words (803989 effective words) took 0.3s, 2854438 effective words/s\n",
      "2024-10-24 11:15:54,233 : INFO : EPOCH 62: training on 949783 raw words (804158 effective words) took 0.3s, 3114228 effective words/s\n",
      "2024-10-24 11:15:54,524 : INFO : EPOCH 63: training on 949783 raw words (804213 effective words) took 0.3s, 2803229 effective words/s\n",
      "2024-10-24 11:15:54,807 : INFO : EPOCH 64: training on 949783 raw words (804061 effective words) took 0.3s, 2891100 effective words/s\n",
      "2024-10-24 11:15:55,055 : INFO : EPOCH 65: training on 949783 raw words (803781 effective words) took 0.2s, 3289939 effective words/s\n",
      "2024-10-24 11:15:55,303 : INFO : EPOCH 66: training on 949783 raw words (803957 effective words) took 0.2s, 3295468 effective words/s\n",
      "2024-10-24 11:15:55,547 : INFO : EPOCH 67: training on 949783 raw words (804177 effective words) took 0.2s, 3356352 effective words/s\n",
      "2024-10-24 11:15:55,785 : INFO : EPOCH 68: training on 949783 raw words (803761 effective words) took 0.2s, 3433231 effective words/s\n",
      "2024-10-24 11:15:56,006 : INFO : EPOCH 69: training on 949783 raw words (803738 effective words) took 0.2s, 3703397 effective words/s\n",
      "2024-10-24 11:15:56,257 : INFO : EPOCH 70: training on 949783 raw words (804374 effective words) took 0.2s, 3261743 effective words/s\n",
      "2024-10-24 11:15:56,491 : INFO : EPOCH 71: training on 949783 raw words (803961 effective words) took 0.2s, 3494830 effective words/s\n",
      "2024-10-24 11:15:56,730 : INFO : EPOCH 72: training on 949783 raw words (804294 effective words) took 0.2s, 3432137 effective words/s\n",
      "2024-10-24 11:15:56,963 : INFO : EPOCH 73: training on 949783 raw words (803961 effective words) took 0.2s, 3505565 effective words/s\n",
      "2024-10-24 11:15:57,216 : INFO : EPOCH 74: training on 949783 raw words (804048 effective words) took 0.2s, 3234241 effective words/s\n",
      "2024-10-24 11:15:57,481 : INFO : EPOCH 75: training on 949783 raw words (804037 effective words) took 0.3s, 3079132 effective words/s\n",
      "2024-10-24 11:15:57,763 : INFO : EPOCH 76: training on 949783 raw words (804121 effective words) took 0.3s, 2896938 effective words/s\n",
      "2024-10-24 11:15:58,027 : INFO : EPOCH 77: training on 949783 raw words (804153 effective words) took 0.3s, 3084990 effective words/s\n",
      "2024-10-24 11:15:58,294 : INFO : EPOCH 78: training on 949783 raw words (804034 effective words) took 0.3s, 3059526 effective words/s\n",
      "2024-10-24 11:15:58,551 : INFO : EPOCH 79: training on 949783 raw words (804452 effective words) took 0.3s, 3176125 effective words/s\n",
      "2024-10-24 11:15:58,814 : INFO : EPOCH 80: training on 949783 raw words (804062 effective words) took 0.3s, 3105859 effective words/s\n",
      "2024-10-24 11:15:59,064 : INFO : EPOCH 81: training on 949783 raw words (803919 effective words) took 0.2s, 3273218 effective words/s\n",
      "2024-10-24 11:15:59,315 : INFO : EPOCH 82: training on 949783 raw words (804215 effective words) took 0.2s, 3253163 effective words/s\n",
      "2024-10-24 11:15:59,623 : INFO : EPOCH 83: training on 949783 raw words (803786 effective words) took 0.3s, 2651727 effective words/s\n",
      "2024-10-24 11:15:59,856 : INFO : EPOCH 84: training on 949783 raw words (804294 effective words) took 0.2s, 3511960 effective words/s\n",
      "2024-10-24 11:16:00,117 : INFO : EPOCH 85: training on 949783 raw words (804069 effective words) took 0.3s, 3128951 effective words/s\n",
      "2024-10-24 11:16:00,362 : INFO : EPOCH 86: training on 949783 raw words (803942 effective words) took 0.2s, 3344041 effective words/s\n",
      "2024-10-24 11:16:00,612 : INFO : EPOCH 87: training on 949783 raw words (804331 effective words) took 0.2s, 3264393 effective words/s\n",
      "2024-10-24 11:16:00,852 : INFO : EPOCH 88: training on 949783 raw words (804143 effective words) took 0.2s, 3418269 effective words/s\n",
      "2024-10-24 11:16:01,093 : INFO : EPOCH 89: training on 949783 raw words (803960 effective words) took 0.2s, 3397900 effective words/s\n",
      "2024-10-24 11:16:01,351 : INFO : EPOCH 90: training on 949783 raw words (804600 effective words) took 0.3s, 3160156 effective words/s\n",
      "2024-10-24 11:16:01,622 : INFO : EPOCH 91: training on 949783 raw words (803807 effective words) took 0.3s, 3006954 effective words/s\n",
      "2024-10-24 11:16:01,885 : INFO : EPOCH 92: training on 949783 raw words (804347 effective words) took 0.3s, 3106673 effective words/s\n",
      "2024-10-24 11:16:02,161 : INFO : EPOCH 93: training on 949783 raw words (804109 effective words) took 0.3s, 2965134 effective words/s\n",
      "2024-10-24 11:16:02,419 : INFO : EPOCH 94: training on 949783 raw words (803999 effective words) took 0.3s, 3168042 effective words/s\n",
      "2024-10-24 11:16:02,675 : INFO : EPOCH 95: training on 949783 raw words (804215 effective words) took 0.3s, 3191809 effective words/s\n",
      "2024-10-24 11:16:02,933 : INFO : EPOCH 96: training on 949783 raw words (803923 effective words) took 0.3s, 3172569 effective words/s\n",
      "2024-10-24 11:16:03,193 : INFO : EPOCH 97: training on 949783 raw words (804050 effective words) took 0.3s, 3130678 effective words/s\n",
      "2024-10-24 11:16:03,431 : INFO : EPOCH 98: training on 949783 raw words (803860 effective words) took 0.2s, 3443650 effective words/s\n",
      "2024-10-24 11:16:03,663 : INFO : EPOCH 99: training on 949783 raw words (804214 effective words) took 0.2s, 3533200 effective words/s\n",
      "2024-10-24 11:16:03,663 : INFO : Word2Vec lifecycle event {'msg': 'training on 94978300 raw words (80414993 effective words) took 24.8s, 3241252 effective words/s', 'datetime': '2024-10-24T11:16:03.663859', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2024-10-24 11:16:03,664 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=82315, vector_size=4, alpha=0.025>', 'datetime': '2024-10-24T11:16:03.664397', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:16:03.725592Z",
     "start_time": "2024-10-24T03:16:03.704022Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.most_similar('法律', topn=10)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('新貌', 0.9997027516365051),\n",
       " ('八德', 0.9996379613876343),\n",
       " ('巴音', 0.9995285272598267),\n",
       " ('武勇', 0.9993701577186584),\n",
       " ('风采录', 0.9991586804389954),\n",
       " ('名作', 0.9991145730018616),\n",
       " ('淅川县', 0.9990727305412292),\n",
       " ('饭团', 0.9990687370300293),\n",
       " ('广播操', 0.9990463256835938),\n",
       " ('劝导', 0.9987432956695557)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:16:03.875947Z",
     "start_time": "2024-10-24T03:16:03.859705Z"
    }
   },
   "cell_type": "code",
   "source": "model1.wv.most_similar('法律', topn=10)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('伊', 0.9996498227119446),\n",
       " ('制造', 0.9993472099304199),\n",
       " ('生产线', 0.9990198612213135),\n",
       " ('渠道', 0.9988608360290527),\n",
       " ('骗局', 0.9987649321556091),\n",
       " ('分销', 0.9985112547874451),\n",
       " ('唱空', 0.9984816312789917),\n",
       " ('第一步', 0.9983130693435669),\n",
       " ('举措', 0.998151421546936),\n",
       " ('操作系统', 0.9981495141983032)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:17:37.816514Z",
     "start_time": "2024-10-24T03:17:37.811185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.nn.parameter import Parameter\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy  # SciPy是基于NumPy开发的高级模块，它提供了许多数学算法和函数的实现\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 余弦相似度函数\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "K = 100  # number of negative samples 负样本随机采样数量\n",
    "C = 3  # nearby words threshold 指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 3  # The number of epochs of training 迭代轮数，default=10\n",
    "MAX_VOCAB_SIZE = 30000  # the vocabulary size 词汇表多大\n",
    "BATCH_SIZE = 32  # the batch size 每轮迭代1个batch的数量\n",
    "LEARNING_RATE = 0.2  # the initial learning rate #学习率\n",
    "EMBEDDING_SIZE = 100  # 词向量维度\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:17:38.961075Z",
     "start_time": "2024-10-24T03:17:38.926075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "with open(\"data/nietzsche.txt\", \"r\") as fin:  # 读入文件\n",
    "    text = fin.read()\n",
    "\n",
    "print('text: ', text[:500])\n",
    "\n",
    "text = [w for w in word_tokenize(text.lower())]\n",
    "# 分词，在这里类似于text.split()\n",
    "\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "# 字典格式，把（MAX_VOCAB_SIZE-1）个最频繁出现的单词取出来，-1是留给不常见的单词\n",
    "\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "# unk表示不常见单词数=总单词数-常见单词数\n",
    "# 这里计算的到vocab[\"<unk>\"]=29999\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()]\n",
    "# 取出字典的所有单词key\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(idx_to_word)}\n",
    "# 取出所有单词的单词和对应的索引，索引值与单词出现次数相反，最常见单词索引为0。\n",
    "\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "# 所有单词的频数values\n",
    "\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "# 所有单词的频率\n",
    "\n",
    "word_freqs = word_freqs ** (3. / 4.)\n",
    "# 论文里乘以3/4次方\n",
    "\n",
    "word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "# 重新计算所有单词的频率\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word)  # 词汇表单词数30000=MAX_VOCAB_SIZE\n",
    "VOCAB_SIZE"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to Truth, have been unskilled and unseemly methods for\n",
      "winning a woman? Certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17683"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:17:39.471239Z",
     "start_time": "2024-10-24T03:17:39.450701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WordDataset(data.Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(WordDataset, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE - 1) for t in text]\n",
    "        # 取出text里每个单词word_to_idx字典里对应的索引,不在字典里返回\"<unk>\"的索引\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)  # 所有单词的总数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_word = self.text_encoded[idx]\n",
    "        # 中心词索引\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
    "        # 周围词索引的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3]\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        # range(idx+1, idx+C+1)超出词汇总数时，需要特别处理，取余数\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # 周围词索引，就是希望出现的正例单词\n",
    "        # print(pos_words)\n",
    "        neg_words = torch.multinomial(\n",
    "            self.word_freqs, K * pos_words.shape[0], True)\n",
    "        # 负例采样单词索引，torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，\n",
    "        # 输出的是self.word_freqs对应的下标。\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。\n",
    "        # 每个正确的单词采样K个，pos_words.shape[0]是正确单词数量\n",
    "        # print(neg_words)\n",
    "        return center_word, pos_words, neg_words\n",
    "\n",
    "\n",
    "dataset = WordDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:17:39.674536Z",
     "start_time": "2024-10-24T03:17:39.669245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.vocab_size = vocab_size  # 字典大小\n",
    "        self.embedding_size = embedding_size  # 编码维度\n",
    "        init = 0.5 / self.embedding_size\n",
    "        self.out_embed = nn.Embedding(\n",
    "            self.vocab_size, self.embedding_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-init, init)\n",
    "        # 权重初始化的一种方法\n",
    "\n",
    "        self.in_embed = nn.Embedding(\n",
    "            self.vocab_size, self.embedding_size, sparse=False)\n",
    "        #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-init, init)\n",
    "        # 权重初始化的一种方法\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        \"\"\"\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labels: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "        \"\"\"\n",
    "        batch_size = input_labels.size(0)\n",
    "\n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        # B * embedding_size\n",
    "        # 这里估计进行了运算：（128,30000）*（30000,100）= 128(B) * 100 (embedding_size)\n",
    "\n",
    "        pos_embedding = self.out_embed(pos_labels)  # B * (2*C) * embedding_size\n",
    "        # 同上，增加了维度(2*C)，表示一个batch有B组周围词单词，一组周围词有(2*C)个单词，每个单词有embedding_size个维度。\n",
    "\n",
    "        # B * (2*C * K) * embedding_size\n",
    "        neg_embedding = self.out_embed(neg_labels)\n",
    "        # 同上，增加了维度(2*C*K)\n",
    "\n",
    "        # torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(\n",
    "            pos_embedding, input_embedding.unsqueeze(2)).squeeze()  # B * (2*C)\n",
    "        log_neg = torch.bmm(\n",
    "            neg_embedding, -input_embedding.unsqueeze(2)).squeeze()  # B * (2*C*K)\n",
    "        # unsqueeze(2)指定位置升维，.squeeze()压缩维度。\n",
    "\n",
    "        # 下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)  # batch_size\n",
    "        loss = log_pos + log_neg\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    def input_embeddings(self):  # 取出self.in_embed数据参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:17:39.904504Z",
     "start_time": "2024-10-24T03:17:39.877830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Embedding(VOCAB_SIZE, EMBEDDING_SIZE).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:18:20.573499Z",
     "start_time": "2024-10-24T03:17:40.097019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch: {}, iter: {}, loss: {}\".format(epoch, i, loss.item()))\n",
    "embedding_weights = model.input_embeddings()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 420.04742431640625\n",
      "epoch: 0, iter: 100, loss: 251.5990753173828\n",
      "epoch: 0, iter: 200, loss: 247.44834899902344\n",
      "epoch: 0, iter: 300, loss: 174.37144470214844\n",
      "epoch: 0, iter: 400, loss: 164.48870849609375\n",
      "epoch: 0, iter: 500, loss: 131.84848022460938\n",
      "epoch: 0, iter: 600, loss: 149.32656860351562\n",
      "epoch: 0, iter: 700, loss: 130.55337524414062\n",
      "epoch: 0, iter: 800, loss: 116.36502075195312\n",
      "epoch: 0, iter: 900, loss: 84.2267074584961\n",
      "epoch: 0, iter: 1000, loss: 121.55711364746094\n",
      "epoch: 0, iter: 1100, loss: 123.77498626708984\n",
      "epoch: 0, iter: 1200, loss: 93.50875854492188\n",
      "epoch: 0, iter: 1300, loss: 88.81300354003906\n",
      "epoch: 0, iter: 1400, loss: 122.85433959960938\n",
      "epoch: 0, iter: 1500, loss: 60.804752349853516\n",
      "epoch: 0, iter: 1600, loss: 122.99179077148438\n",
      "epoch: 0, iter: 1700, loss: 83.84132385253906\n",
      "epoch: 0, iter: 1800, loss: 122.31605529785156\n",
      "epoch: 0, iter: 1900, loss: 125.36405944824219\n",
      "epoch: 0, iter: 2000, loss: 96.53458404541016\n",
      "epoch: 0, iter: 2100, loss: 85.01092529296875\n",
      "epoch: 0, iter: 2200, loss: 86.52742004394531\n",
      "epoch: 0, iter: 2300, loss: 84.50567626953125\n",
      "epoch: 0, iter: 2400, loss: 88.94547271728516\n",
      "epoch: 0, iter: 2500, loss: 140.65721130371094\n",
      "epoch: 0, iter: 2600, loss: 84.87350463867188\n",
      "epoch: 0, iter: 2700, loss: 63.352684020996094\n",
      "epoch: 0, iter: 2800, loss: 88.15921020507812\n",
      "epoch: 0, iter: 2900, loss: 73.26068878173828\n",
      "epoch: 0, iter: 3000, loss: 84.10123443603516\n",
      "epoch: 1, iter: 0, loss: 44.42802429199219\n",
      "epoch: 1, iter: 100, loss: 37.032588958740234\n",
      "epoch: 1, iter: 200, loss: 43.89025115966797\n",
      "epoch: 1, iter: 300, loss: 40.82145690917969\n",
      "epoch: 1, iter: 400, loss: 39.88592529296875\n",
      "epoch: 1, iter: 500, loss: 39.241966247558594\n",
      "epoch: 1, iter: 600, loss: 37.3780403137207\n",
      "epoch: 1, iter: 700, loss: 41.56163024902344\n",
      "epoch: 1, iter: 800, loss: 35.855560302734375\n",
      "epoch: 1, iter: 900, loss: 39.01957321166992\n",
      "epoch: 1, iter: 1000, loss: 37.969993591308594\n",
      "epoch: 1, iter: 1100, loss: 36.704750061035156\n",
      "epoch: 1, iter: 1200, loss: 35.084808349609375\n",
      "epoch: 1, iter: 1300, loss: 36.053260803222656\n",
      "epoch: 1, iter: 1400, loss: 33.77030944824219\n",
      "epoch: 1, iter: 1500, loss: 37.58765411376953\n",
      "epoch: 1, iter: 1600, loss: 37.359710693359375\n",
      "epoch: 1, iter: 1700, loss: 34.87610626220703\n",
      "epoch: 1, iter: 1800, loss: 33.8552360534668\n",
      "epoch: 1, iter: 1900, loss: 36.27161407470703\n",
      "epoch: 1, iter: 2000, loss: 35.10383224487305\n",
      "epoch: 1, iter: 2100, loss: 33.25946044921875\n",
      "epoch: 1, iter: 2200, loss: 36.50238037109375\n",
      "epoch: 1, iter: 2300, loss: 36.99341583251953\n",
      "epoch: 1, iter: 2400, loss: 42.14544677734375\n",
      "epoch: 1, iter: 2500, loss: 35.85223388671875\n",
      "epoch: 1, iter: 2600, loss: 36.53776550292969\n",
      "epoch: 1, iter: 2700, loss: 34.149810791015625\n",
      "epoch: 1, iter: 2800, loss: 40.60791015625\n",
      "epoch: 1, iter: 2900, loss: 35.2977294921875\n",
      "epoch: 1, iter: 3000, loss: 39.60332489013672\n",
      "epoch: 2, iter: 0, loss: 32.930137634277344\n",
      "epoch: 2, iter: 100, loss: 33.54831314086914\n",
      "epoch: 2, iter: 200, loss: 32.83049774169922\n",
      "epoch: 2, iter: 300, loss: 33.171512603759766\n",
      "epoch: 2, iter: 400, loss: 33.25475311279297\n",
      "epoch: 2, iter: 500, loss: 33.145957946777344\n",
      "epoch: 2, iter: 600, loss: 33.169677734375\n",
      "epoch: 2, iter: 700, loss: 32.86514663696289\n",
      "epoch: 2, iter: 800, loss: 34.6063232421875\n",
      "epoch: 2, iter: 900, loss: 33.09807205200195\n",
      "epoch: 2, iter: 1000, loss: 33.49464416503906\n",
      "epoch: 2, iter: 1100, loss: 34.346717834472656\n",
      "epoch: 2, iter: 1200, loss: 32.67115020751953\n",
      "epoch: 2, iter: 1300, loss: 33.36592102050781\n",
      "epoch: 2, iter: 1400, loss: 33.35350036621094\n",
      "epoch: 2, iter: 1500, loss: 33.5130615234375\n",
      "epoch: 2, iter: 1600, loss: 32.24302673339844\n",
      "epoch: 2, iter: 1700, loss: 32.82262420654297\n",
      "epoch: 2, iter: 1800, loss: 32.92860412597656\n",
      "epoch: 2, iter: 1900, loss: 33.569801330566406\n",
      "epoch: 2, iter: 2000, loss: 33.317161560058594\n",
      "epoch: 2, iter: 2100, loss: 32.52354049682617\n",
      "epoch: 2, iter: 2200, loss: 33.26710510253906\n",
      "epoch: 2, iter: 2300, loss: 33.09550476074219\n",
      "epoch: 2, iter: 2400, loss: 33.052589416503906\n",
      "epoch: 2, iter: 2500, loss: 33.227691650390625\n",
      "epoch: 2, iter: 2600, loss: 33.18009567260742\n",
      "epoch: 2, iter: 2700, loss: 32.353675842285156\n",
      "epoch: 2, iter: 2800, loss: 33.10731506347656\n",
      "epoch: 2, iter: 2900, loss: 32.577606201171875\n",
      "epoch: 2, iter: 3000, loss: 33.50678253173828\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:18:20.694726Z",
     "start_time": "2024-10-24T03:18:20.578571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def evaluate(filename, embedding_weights):\n",
    "    if not os.path.isfile(filename):\n",
    "        return\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    print(data.head())\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[\n",
    "                word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(\n",
    "                float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    # model_similarity\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)\n",
    "\n",
    "\n",
    "def find_nearest(word):\n",
    "    if word not in word_to_idx:\n",
    "        return\n",
    "    index = word_to_idx.get(word, 0)\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding)\n",
    "                        for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]\n",
    "\n",
    "\n",
    "print(\"simlex-999\", evaluate(\"data/en-simlex-999.txt\", embedding_weights))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     old          new  1.58\n",
      "0  smart  intelligent  9.20\n",
      "1   hard    difficult  8.77\n",
      "2  happy     cheerful  9.55\n",
      "3   hard         easy  0.95\n",
      "4   fast        rapid  8.75\n",
      "simlex-999 SignificanceResult(statistic=0.08471268321334338, pvalue=0.11261446417779314)\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:18:22.114806Z",
     "start_time": "2024-10-24T03:18:20.824137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in [\"good\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'other', 'life', 'bad', 'first', 'only', 'things', 'present', 'almost', 'being']\n",
      "green ['green', 'infinite,--when', 'calmest', '_operari_,', 'dissection,', 'travesty', 'advantageous', 'selfishness', 'bath,', 'no-more-weeping']\n",
      "like ['like', 'after', 'before', 'free', 'thereby', 'thus', 'against', 'again', 'call', 'makes']\n",
      "america None\n",
      "chicago None\n",
      "work ['work', 'error', 'desires', 'schopenhauer', 'feelings', 'europe,', 'master', 'future', 'common', 'sacrifice']\n",
      "computer None\n",
      "language ['language', 'sense,', 'saint', '\"good\"', 'extraordinary', 'distinction', 'beautiful', 'hatred', 'path', 'significance']\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:18:22.394218Z",
     "start_time": "2024-10-24T03:18:22.131922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "man_idx = word_to_idx[\"man\"]\n",
    "king_idx = word_to_idx[\"king\"]\n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gained--and\n",
      "omit\n",
      "aristophanes--that\n",
      "possess--and\n",
      "circumlocution:\n",
      "different?\n",
      "preachers\n",
      "move,\n",
      "break,\n",
      "signify,\n",
      "loved?\n",
      "little.\n",
      "hunger,\n",
      "affected,\n",
      "inspirited.\n",
      "do!\"--i\n",
      "scoffers\n",
      "dig\n",
      "wiser\n",
      "health).\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
