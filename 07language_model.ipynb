{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T06:05:27.077117Z",
     "start_time": "2024-10-25T06:05:27.074033Z"
    }
   },
   "source": [
    "import math\n",
    "from shutil import make_archive\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:05:27.127651Z",
     "start_time": "2024-10-25T06:05:27.124282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ],
   "id": "3017bcb29e2d7450",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe"
   ],
   "id": "ab3d7ec74b8ea5bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:05:27.174366Z",
     "start_time": "2024-10-25T06:05:27.172052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.Q_matrix = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.K_matrix = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V_matrix = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.out_matrix = nn.Linear(d_v * num_heads, d_model)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Q.weight)\n",
    "        nn.init.xavier_uniform_(self.K.weight)\n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        n = q.size(0)\n",
    "        q_len, k_len = q.size(1), k.size(1)\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        num_heads = self.num_heads\n",
    "        q = self.Q_matrix(q).view(n, -1, num_heads, d_k).transpose(1, 2)\n",
    "        k = self.K_matrix(k).view(n, -1, num_heads, d_k).transpose(1, 2)\n",
    "        v = self.V_matrix(v).view(n, -1, num_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            assert mask.size() == (n, q_len, k_len)\n",
    "            mask = mask.unsqueeze(1).repeat(1, num_heads, 1, 1)\n",
    "            mask = mask.bool()\n",
    "            scores.masked_fill_(mask, -float('inf'))\n",
    "        attentions = F.softmax(scores, dim=-1)\n",
    "        attentions = self.dropout(attentions)\n",
    "        output = torch.matmul(attentions, v)\n",
    "        output = output.transpose(1, 2).contiguous().reshape(n, -1, d_v * num_heads)\n",
    "        output = self.out_matrix(output)\n",
    "        return output"
   ],
   "id": "7aaf179ff4ad380b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
